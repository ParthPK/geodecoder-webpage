<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CityGuessr, a model for worldwide video geolocalization along with the CityGuessr68k dataset.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CityGuessr: City-Level Video Geo-Localization on a Global Scale</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CityGuessr: City-Level Video Geo-Localization on a Global Scale</h1>
          <h2 class="subtitle conference">European Conference on Computer Vision(ECCV) 2024</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=OHC7c90AAAAJ&hl=en">Parth Parag Kulkarni</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://iitr.ac.in/Schools/Mehta%20Family%20School%20of%20Data%20Science%20and%20Artificial%20Intelligence/People/Faculty/101043.html">Gaurav Kumar Nayak</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Center for Research in Computer Vision, University of Central Florida, USA,</span>
            <span class="author-block"><sup>2</sup>Mehta Family School of DS & AI, Indian Institute of Technology Roorkee, India</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1xomm0v6VdguZT4brC3_KTX5Opqkj8eCE/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1xomm0v6VdguZT4brC3_KTX5Opqkj8eCE/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://parthpk.github.io/cityguessr-webpage/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ParthPK/CityGuessr"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://www.crcv.ucf.edu/data1/CityGuessr68k/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Abu_Dhabi.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">Abu Dhabi, UAE</div>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Wellington.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">Wellington, New Zealand</div>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Cairo.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">Cairo, Egypt</div>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Stockholm.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">Stockholm, Sweden</div>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/New_Orleans.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">New Orleans, USA</div>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Lima.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">Lima, Peru</div>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Busan.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">Busan, South Korea</div>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Johannesburg.mp4"
                    type="video/mp4">
          </video>
          <div class="caption">Johannesburg, South Africa</div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video geolocalization is a crucial problem in current times.  Given just a video, ascertaining where it was captured  
            from can have a plethora of advantages. The problem of worldwide geolocalization has been tackled before, but only 
            using the image modality. Its video counterpart remains relatively unexplored. Meanwhile, video geolocalization has 
            also garnered some attention in the recent past, but the existing methods are all restricted to specific regions. 
            This motivates us to explore the problem of video geolocalization at a global scale. Hence, we propose a novel problem 
            of worldwide video geolocalization with the objective of hierarchically predicting the correct city, state/province, 
            country, and continent, given a video. However, no large scale video datasets that have extensive worldwide coverage 
            exist, to train models for solving this problem. To this end, we introduce a new dataset, "<i>CityGuessr68k</i>" 
            comprising of 68,269 videos from 166 cities all over the world. We also propose a novel baseline approach to this 
            problem, by designing a transformer-based architecture comprising of an elegant "<i>Self-Cross Attention</i>" module 
            for incorporating scenes as well as a "<i>TextLabel Alignment</i>" strategy for distilling knowledge from textlabels 
            in feature space. To further enhance our location prediction, we also utilize soft-scene labels. Finally we demonstrate 
            the performance of our method on our new dataset as well as Mapillary(MSLS).
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">CityGuessr68k Dataset</h2>
        <div class="publication-video" style="padding-bottom: 31.25%;">
          <img src="static/images/cityguessr_v_msls.png" />
        </div>
        <div class="content has-text-justified">
          <p>
            The CityGuessr68k dataset consists of 68,269 first-person driving and walking videos from 166 cities, 157 states/provinces, 91 
            countries and 6 continents. Each video is annotated with hierarchical location labels in the form of its continent, country, 
            state/province, city. As we see in the figure below, CityGuessr68k has a good geographical coverage. Along with that, frequency 
            distribution among classes is also relatively even. Each video is divided into frames of resolution 1280x720, which is higher 
            than the only other worldwide image sequence dataset, Mapillary(MSLS) and all videos are approximately same in length. Our 
            dataset is approximately 5x larger than Mapillary(MSLS) and spread across more cities around the world.
          </p>  
        </div>
      </div>
    </div>
    <!--/ Dataset. -->

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="publication-video">
          <img src="static/images/Method_page-0001.jpg" />
        </div>
        <div class="content has-text-justified">
          <p>
            VideoMAE encoder outputs feature embeddings of the input video. The embeddings are then passed into 4 classifiers pertaining 
            to 4 hierarchies. Their predictions are used for computing Geolocalization loss. Simultaneously prediction vectors are input 
            into the Self-Cross Attention module, where vectors of all 4 hierarchies are concatenated and are attended to, by themselves 
            and by each other to generate an intermediate attended vector(PV'). In the attention weights(w), the single colored weights 
            along the diagonal refer to self attention weights, while the gradient double colored weights are the cross attention weights 
            between vectors of those two different hierarchies. PV' is passed simultaneously through FFN<sub>s</sub> to generate vector 
            PV'<sub>s</sub> for Scene loss computation, and to the TextLabel Alignment module. There, it is passed through FFN<sub>t</sub>
            to generate vector PV'<sub>t</sub>. PV'<sub>t</sub> is used for TextLabel Alignment with feature embeddings F<sub>t</sub> 
            generated by the pretrained text-encoder from the label names of all 4 hierarchies.
          </p>  
        </div>
      </div>
    </div>
    <!--/ Method. -->

    <!-- Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="publication-video" style="padding-bottom: 20.25%;">
          <img src="static/images/res_cg.jpg" />
        </div>
        <div class="content has-text-justified">
          <p>
            The first table shows the impact of our 2 proposed modules. Addition of Self-Cross Attention module certainly helps the model to 
            train better and gives better validation performance. We also showcase our results on two variations of scene labels, one 
            obtained by majority voting and other with soft labels. Comparing their performance, we see that soft labels are more helpful in 
            model training. Table also shows that incorporation of the TextLabel Alignment strategy enhances the features of the model, thus 
            giving a better performance. We showcase our results on two variations, text embeddings from city labels, and from mean of 
            features from all hierarchy labels. Comparing their performance, we see that using all hierarchies helps the model train better 
            as hypothesized.
          </p>  
          <p>
            As no worldwide video geolocalization methods exist, we compare our model to the baselines with TimesFormer and VideoMAE 
            encoders, along with the relevant state-of-the-art image geolocalization methods. For image models, we use a random frame from 
            the video for geolocalization. Hierarchy classifiers are included for all models, and everything else is kept the same. The next
            table shows the results of our model on our dataset. Our model is able to achieve a 69.6% top1 accuracy on City prediction, i.e.,
            the most fine-grained hierarchy. Our model showcases an improvement of ∼ 6% highlighting the significance of our modules. Our 
            model also shows an improvement in the coarser hierarchies with an ∼ 6% jump in state/province prediction, an ∼ 5% improvement 
            in country and a ∼ 4% in continent prediction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Dataset. -->
  </div>
</section>
<!--
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

     
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      

     
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    

    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        

        
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        

      </div>
    </div>
    


    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>
-->  

</body>
</html>
